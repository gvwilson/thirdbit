---
title: "Code Review Tools"
date: 2007-04-25 07:58:57
year: 2007
---
We're two weeks away from starting another round of revisions on the Online Marking Tool (<a href="https://www.drproject.org/olm">OLM</a>), so I've started catching up with other code review tools.  So far, I've looked at:
<ul>
	<li><a href="http://www.cenqua.com/crucible/">Crucible</a>, from Cenqua (via <a href="http://www.geocities.com/mattdoar/">Matt Doar</a>)</li>
	<li><a href="http://smartbear.com/codecollab.php">Code Collaborator</a>, from SmartBear</li>
	<li><a href="http://www.niallkennedy.com/blog/archives/2006/11/google-mondrian.html">Mondrian</a>, at Google</li>
</ul>
and I'd welcome pointers to others that people have actually used.  The most interesting find by far has been <a href="http://smartbearsoftware.com/codecollab-code-review-book.php"><em>Best Kept Secrets of Peer Code Review</em></a>, a free (if you're in the US; cheap elsewhere) book from SmartBear that includes <a href="http://smartbearsoftware.com/docs/book/code-review-literature.pdf">an excellent summary of empirical data on the effectiveness of code reviews</a>, explains what's wrong with the heavyweight approach pioneered by Michael Fagan at IBM in the 1970s, and then presents results from <a href="http://smartbearsoftware.com/docs/book/code-review-cisco-case-study.pdf">a large study done at Cisco</a> showing that lightweight peer reviews of code are just as effective, but much less costly. Yes, they're trying to convince you to buy their software, but the data really is on their side.  The question now is, where and how do we integrate this into undergrad teaching?
