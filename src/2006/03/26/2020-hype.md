---
title: "2020 Hype"
date: 2006-03-26
---
A report from <a href="http://research.microsoft.com">Microsoft Research</a> called <a href="http://research.microsoft.com/towards2020science/">2020 Science</a> got a lot of press this week: <a href="http://www.nature.com/nature/focus/futurecomputing/index.html">Nature</a> seems to think it's the biggest story of the year so far, and <a href="http://www.economist.com/displaystory.cfm?story_id=5655067">The Economist</a> gave it three full columns.  Sadly, amidst the gush about how computers are revolutionizing science, no one mentions that most scientists have no idea how reliable their programs areâ€”in fact, most scientists don't even know how they would figure that out [1,2].  If someone submitting a paper to Nature said, "We didn't calibrate the equipment, we didn't write down the settings, and we have no idea what the error bars on our graphs should be," their work would be bounced without a second thought.  Unless computational scientists decide to live up to those standards, the "revolution" that <a href="http://research.microsoft.com/towards2020science/">2020 Science</a> describes will be a long time coming.

[1] <a href="http://www.americanscientist.org/template/AssetDetail/assetid/48548">"Where's the Real Bottleneck in Scientific Computing?"</a>

[2] <a href="http://www.highproductivity.org/vol58no1p35_41.pdf">"Computational Science Demands a New Paradigm"</a>.
